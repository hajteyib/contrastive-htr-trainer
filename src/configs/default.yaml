# default.yaml

# --- Configuration de l'Expérience ---
experiment:
  name: "htr_contrastive_final_stable_run"
  description: "Final, stable pre-training run on 60k images with SimCLR augmentations and strong regularization."
  version: "7.0"

# --- Architecture du Modèle ---
model:
  global_dim: 512
  patch_dim: 128
  fpn_channels: 256

# --- Données ---
data:
  # Chemin vers le fichier texte maître.
  data_list_file: "/home/jovyan/buckets/ehsebou-manuscrits/final_valid_paths.txt"
  
  # Hauteur cible pour le RandomResizedCrop
  target_height: 400
  
  batch_size: 4
  num_workers: 8
  pin_memory: true

# --- Augmentations ---
augmentation:
  # Force d'augmentation élevée pour combattre l'overfitting.
  strength: 0.8

# --- Fonction de Coût (Loss) ---
loss:
  # Températures stables pour éviter les NaN avec AMP.
  global_temp: 0.2
  patch_temp: 0.2
  
  lambda_global: 1.0
  lambda_patch: 0.8
  lambda_style: 0.3
  lambda_semantic: 0.0 # Gardé à 0.

  queue_size: 8192
  use_hard_mining: true
  
# --- Entraînement ---
training:
  num_epochs: 50
  learning_rate: 5e-4 
  # Régularisation forte.
  weight_decay: 1e-3
  momentum: 0.996
  warmup_epochs: 5
  
  # --- STABILITÉ & VITESSE ---
  # On active AMP, car nos températures sont maintenant sûres.
  use_amp: true
  
  gradient_accumulation_steps: 5
  gradient_clip_norm: 1.0
  validate_every_n_epochs: 1
  save_every_n_epochs: 5
  
  early_stopping:
    enabled: true
    patience: 10
    # On suit la NOUVELLE métrique de précision, qui est plus significative.
    monitor: "val_contrastive_acc"
    # On cherche à la MAXIMISER.
    mode: "max"

# --- Optimiseur ---
optimizer:
  type: "adamw"
  betas: [0.9, 0.999]
  eps: 1e-8

# --- Scheduler de Learning Rate ---
scheduler:
  type: "cosine"
  min_lr: 1e-6

# --- Paramètres Avancés ---
advanced:
  # --- STABILITÉ ---
  # On désactive torch.compile, qui est la source confirmée de nos bugs CUDAGraphs.
  # La stabilité est prioritaire sur ce gain de vitesse.
  compile_model: false
  compile_mode: "reduce-overhead"
  channels_last_memory_format: true

# --- Presets ---
presets:
  full_training: {}